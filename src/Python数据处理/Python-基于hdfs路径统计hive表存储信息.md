><p style="font-family: 'Microsoft YaHei', sans-serif; line-height: 1.5;">
>ä½œè€…ï¼šæ•°æ®äººé˜¿å¤š
></p>

# èƒŒæ™¯
ä½ çš„Hiveæ•°æ®åº“æ˜¯ä¸æ˜¯ä¹Ÿæ‚„æ‚„â€œè†¨èƒ€â€äº†ï¼Ÿ

å¼€å‘è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å„ç§ä¸­é—´è¡¨ï¼Œæ—¥ç§¯æœˆç´¯ï¼Œå ç”¨äº†å¤§é‡å­˜å‚¨ç©ºé—´ï¼Œæ‰‹åŠ¨æ¸…ç†æ—¶å´çŠ¯äº†éš¾ï¼š

- **å“ªäº›è¡¨æœ€å ç©ºé—´ï¼Ÿ**

- **å“ªäº›è¡¨æ—©å·²è¿‡æ—¶ï¼Ÿ**

- **è°åˆ›å»ºçš„ï¼Ÿèƒ½ä¸èƒ½åˆ ï¼Ÿ**

ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘å†™äº†ä¸€ä¸ªPythonè„šæœ¬ï¼Œèƒ½ä¸€é”®ç”Ÿæˆç»Ÿè®¡æŠ¥è¡¨ï¼Œæ¸…æ™°åˆ—å‡ºï¼š

ğŸ“Š **è¡¨å­˜å‚¨å¤§å°**

ğŸ“… **æœ€åä¿®æ”¹æ—¶é—´**

ğŸ‘¨ğŸ’» **è¡¨ä½¿ç”¨äºº**

è®©ä½ å¯¹åº“è¡¨æƒ…å†µäº†å¦‚æŒ‡æŒï¼Œç²¾å‡†æ¸…ç†ï¼Œå½»åº•å‘Šåˆ«å­˜å‚¨ç„¦è™‘ï¼

# å°ç¼–ç¯å¢ƒ
```python
import sys

print('python ç‰ˆæœ¬ï¼š',sys.version.split('|')[0])
#python ç‰ˆæœ¬ï¼š 3.11.11
```
# ç»Ÿè®¡ç»“æœç¤ºä¾‹
![ç»“æœç¤ºä¾‹](./images/6641583-93b0ece5db19bce3.png)
# å®Œæ•´ä»£ç 
```
"""
Hiveè¡¨å­˜å‚¨ä¿¡æ¯ç»Ÿè®¡è„šæœ¬ - åŸºäºHDFSè·¯å¾„æ£€æŸ¥
"""

import subprocess
import re
import datetime
import logging
import sys
import os
from typing import Dict, List, Tuple, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("hive_table_stats_hdfs.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class HiveTableStatsHDFS:
    def __init__(self, hdfs_cmd: str = "hadoop fs"):
        self.hdfs_cmd = hdfs_cmd
        # self.warehouse_base = "/user/hive/warehouse"
        
    def execute_hdfs_command(self, command: str) -> Tuple[bool, str]:
        """æ‰§è¡ŒHDFSå‘½ä»¤"""
        try:
            cmd = f"{self.hdfs_cmd} {command}"
            result = subprocess.run(
                cmd, 
                shell=True, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE, 
                universal_newlines=True,
                timeout=60
            )
            
            if result.returncode == 0:
                return True, result.stdout.strip()
            else:
                logger.error(f"HDFSå‘½ä»¤å¤±è´¥: {result.stderr}")
                return False, result.stderr
                
        except subprocess.TimeoutExpired:
            logger.error(f"HDFSå‘½ä»¤è¶…æ—¶: {command}")
            return False, "Command timeout"
        except Exception as e:
            logger.error(f"æ‰§è¡ŒHDFSå‘½ä»¤æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return False, str(e)
    
    def get_tables_from_database(self, db_path: str) -> List[str]:
        """è·å–æŒ‡å®šæ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨"""
        logger.info(f"è·å–æ•°æ®åº“ {db_path} ä¸­çš„è¡¨åˆ—è¡¨...")
        success, output = self.execute_hdfs_command(f"-ls {db_path}")
        
        if not success:
            logger.warning(f"æ— æ³•è®¿é—®æ•°æ®åº“è·¯å¾„: {db_path}")
            return []
        
        tables = []
        for line in output.split('\n'):
            if line.startswith('d'):
                parts = line.split()
                if len(parts) >= 8:
                    table_path = parts[-1]
                    table_name = table_path.split('/')[-1]
                    tables.append((table_name, table_path))
        
        logger.info(f"åœ¨ {db_path} ä¸­æ‰¾åˆ° {len(tables)} å¼ è¡¨")
        return tables
    
    def get_table_size(self, table_path: str) -> Optional[str]:
        """è·å–è¡¨çš„å­˜å‚¨å¤§å°"""
        success, output = self.execute_hdfs_command(f"-du -s -h {table_path}")
        
        if not success:
            return None
        
        # è§£æè¾“å‡ºï¼Œè·å–æ€»å¤§å°
        if output:
            parts = output.split()
            if len(parts) == 5:
                return parts[0], parts[1]  # è¿”å›äººç±»å¯è¯»çš„å¤§å°ã€å­—èŠ‚ç±»å‹
            
            if len(parts) == 3:
                return parts[0], 'B'  # è¿”å›äººç±»å¯è¯»çš„å¤§å°ã€å­—èŠ‚ç±»å‹
        
        return None
    
    def get_table_details(self, table_path: str) -> Tuple[Optional[str], Optional[str], bool]:
        """è·å–è¡¨çš„è¯¦ç»†ä¿¡æ¯ï¼šæœ€åä¿®æ”¹æ—¶é—´ã€ä¿®æ”¹äººå‘˜ã€æ˜¯å¦åˆ†åŒºè¡¨"""
        success, output = self.execute_hdfs_command(f"-ls -t {table_path} | head -n 5")
        
        if not success:
            return None, None, False
        
        lines = output.strip().split('\n')
        if not lines:
            return None, None, False
        
        # è·å–ç¬¬ä¸€æ¡è®°å½•ï¼ˆæœ€æ–°çš„ï¼‰
        if 'Found' in lines[0]:
            first_line = lines[1]
        else:
            first_line = lines[0]
        
        parts = first_line.split()
        
        if len(parts) < 8:
            return None, None, False
        
        # è§£æä¿®æ”¹æ—¶é—´å’Œä¿®æ”¹äººå‘˜
        mod_time = f"{parts[5]} {parts[6]}"
        modifier = parts[2]  # æ–‡ä»¶æ‰€æœ‰è€…
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†åŒºè¡¨ï¼šæŸ¥çœ‹è¡¨ç›®å½•ä¸‹æ˜¯å¦æœ‰å­ç›®å½•
        is_partitioned = False
        if parts[0].startswith('d'):
            # å¦‚æœæœ‰å­ç›®å½•ï¼Œåˆ™è®¤ä¸ºæ˜¯åˆ†åŒºè¡¨
            is_partitioned = True
        
        return modifier, mod_time, is_partitioned
    
    def get_all_table_stats(self, specific_database: str = None) -> List[Dict]:
        """è·å–æ‰€æœ‰è¡¨çš„ç»Ÿè®¡ä¿¡æ¯"""
        results = []
        
        # æ•°æ®åº“åå­—
        db_name = specific_database.split('/')[-1].replace('.db','')
        
        # è·å–æ•°æ®åº“è·¯å¾„
        tables = self.get_tables_from_database(specific_database)
        total_tables = len(tables)
        
        logger.info(f"å¼€å§‹å¤„ç†æ•°æ®åº“ {db_name} ä¸­çš„ {len(tables)} å¼ è¡¨...")
            
        for i, (table_name, table_path) in enumerate(tables, 1):
            logger.info(f"å¤„ç†è¡¨ [{i}/{len(tables)}]: {db_name}.{table_name}")
            
            try:
                # è·å–è¡¨å¤§å°
                size_1, size_2 = self.get_table_size(table_path)
                
                # è·å–è¡¨è¯¦ç»†ä¿¡æ¯
                modifier, mod_time, is_partitioned = self.get_table_details(table_path)
                
                table_info = {
                    'table_name': f'{db_name}.{table_name}',
                    'storage_size_1': float(size_1),
                    'storage_size_2': size_2,
                    'storage_location': table_path,
                    'last_modification_date': mod_time or "æœªçŸ¥",
                    'last_modifier': modifier or "æœªçŸ¥",
                    'is_partitioned': 'æ˜¯' if is_partitioned else 'å¦'
                }
                
                results.append(table_info)
                
            except Exception as e:
                logger.error(f"å¤„ç†è¡¨ {db_name}.{table_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                error_info = {
                    'database': db_name,
                    'table_name': table_name,
                    'storage_size': 'é”™è¯¯',
                    'storage_location': table_path,
                    'last_modification_date': 'é”™è¯¯',
                    'last_modifier': 'é”™è¯¯',
                    'is_partitioned': 'é”™è¯¯'
                }
                results.append(error_info)
        
        logger.info(f"æ€»å…±å¤„ç†äº† {len(results)} å¼ è¡¨")
        return results
    
    def export_to_excel(self, data: List[Dict], filename: str = None):
        """å¯¼å‡ºç»“æœåˆ°Excelæ–‡ä»¶"""        
        try:
            import pandas as pd
            
            pd_data = pd.DataFrame(data)
            
            # æŒ‰å¤§å°è¿›è¡Œæ’åº
            key_type={'T':1,'G':2,'M':3,'K':4}
            # åˆ›å»ºä¸´æ—¶åˆ—ç”¨äºæ’åº
            pd_data['temp_storage_size_2'] = pd_data['storage_size_2'].map(key_type)
            pd_data_sorted = pd_data.sort_values(
                by=['temp_storage_size_2','storage_size_1'],
                ascending=[True,False]
            )
            # åˆ é™¤ä¸´æ—¶åˆ—
            pd_data_sorted = pd_data_sorted.drop('temp_storage_size_2', axis=1)
            
            # é‡å‘½åå­—æ®µ
            dic_columns={
                "table_name":"è¡¨å",
                "storage_size_1":"å¤§å°-1",
                "storage_size_2":"å¤§å°-2",
                "storage_location":"å­˜å‚¨ä½ç½®",
                "last_modification_date":"æœ€åä¸€æ¬¡ä¿®æ”¹æ—¶é—´",
                "last_modifier":"äººå‘˜",
                "is_partitioned":"æ˜¯å¦åˆ†åŒº"
            }
            pd_data_sorted_rename = pd_data_sorted.rename(columns=dic_columns)
            pd_data_sorted_rename.to_excel(filename, index=False, )
            
            logger.info(f"ç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
            return True
        except ModuleNotFoundError as e:
            logger.error(f"æ²¡æœ‰å®‰è£… pandas : {e}")
            return False
        except Exception as e:
            logger.error(f"å¯¼å‡ºå¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""  
    # æ‰‹åŠ¨æŒ‡å®šæ•°æ®åº“ç›®å½•
    specific_db = '/user/hive/warehouse/warehouse.db'
    
    # åˆ›å»ºç»Ÿè®¡å¯¹è±¡
    stats = HiveTableStatsHDFS()
    
    try:
        # è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯
        print(f"\nå¼€å§‹ç»Ÿè®¡è¡¨ä¿¡æ¯...")
        results = stats.get_all_table_stats(specific_db)        
        
        # å¯¼å‡ºåˆ°Excel
        stats.export_to_excel(results, 'hdfs_hive_table_statistics.xlsx')
        
        print("\nç»Ÿè®¡å®Œæˆ! è¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ 'hive_table_stats_hdfs.log' è·å–è¯¦ç»†ä¿¡æ¯")
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\næ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logger.exception("ä¸»ç¨‹åºå¼‚å¸¸")


if __name__ == "__main__":
    main()
```
# å†å²ç›¸å…³æ–‡ç« 
- [Python-åŸºäºpyhiveåº“æ“ä½œhive](/Pythonæ•°æ®å¤„ç†/Python-åŸºäºpyhiveåº“æ“ä½œhive.md)
- [Python-åˆ©ç”¨pandaså¯¹æ•°æ®è¿›è¡Œç‰¹å®šæ’åº](/Pythonæ•°æ®å¤„ç†/Python-åˆ©ç”¨pandaså¯¹æ•°æ®è¿›è¡Œç‰¹å®šæ’åº.md)
- [hadoop-å¸¸ç”¨å‘½ä»¤æ€»ç»“](/Hive/hadoop-å¸¸ç”¨å‘½ä»¤æ€»ç»“.md)

**************************************************************************
**ä»¥ä¸Šæ˜¯è‡ªå·±å®è·µä¸­é‡åˆ°çš„ä¸€äº›é—®é¢˜ï¼Œåˆ†äº«å‡ºæ¥ä¾›å¤§å®¶å‚è€ƒå­¦ä¹ ï¼Œæ¬¢è¿å…³æ³¨å¾®ä¿¡å…¬ä¼—å·ï¼šDataShare ï¼Œä¸å®šæœŸåˆ†äº«å¹²è´§**
